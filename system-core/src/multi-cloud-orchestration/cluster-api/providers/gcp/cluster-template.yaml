# GCP Cluster Template for Multi-Cloud Orchestration
# This defines the complete cluster configuration for GCP
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: trustram-gcp-cluster
  namespace: default
  labels:
    cluster.x-k8s.io/cluster-name: trustram-gcp-cluster
    environment: production
    cloud-provider: gcp
    trustram-version: v4.4
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
    services:
      cidrBlocks:
      - 10.96.0.0/12
    serviceDomain: cluster.local
  
  # Infrastructure Reference
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: GCPCluster
    name: trustram-gcp-cluster
  
  # Control Plane Configuration
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: trustram-gcp-control-plane

---
# GCP Infrastructure Cluster
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: GCPCluster
metadata:
  name: trustram-gcp-cluster
  namespace: default
spec:
  # GCP Project Configuration
  project: "trustram-v44-project"  # Replace with actual project ID
  region: "us-west1"
  
  # Network Configuration
  network:
    name: "trustram-network"
    autoCreateSubnetworks: false
    
  # Subnet Configuration for High Availability
  subnets:
  - name: "trustram-control-plane-subnet"
    cidrBlock: "10.2.0.0/24"
    region: "us-west1"
    description: "Subnet for control plane nodes"
    enableFlowLogs: true
    privateGoogleAccess: true
    purpose: "PRIVATE"
    logConfig:
      aggregationInterval: "INTERVAL_5_SEC"
      flowSampling: 0.5
      metadata: "INCLUDE_ALL_METADATA"
  - name: "trustram-worker-subnet"
    cidrBlock: "10.2.1.0/24"
    region: "us-west1"
    description: "Subnet for worker nodes"
    enableFlowLogs: true
    privateGoogleAccess: true
    purpose: "PRIVATE"
    logConfig:
      aggregationInterval: "INTERVAL_5_SEC"
      flowSampling: 0.5
      metadata: "INCLUDE_ALL_METADATA"
  - name: "trustram-pods-subnet"
    cidrBlock: "10.2.2.0/24"
    region: "us-west1"
    description: "Secondary subnet for pods"
    enableFlowLogs: false
    privateGoogleAccess: true
    purpose: "PRIVATE"
  
  # Firewall Rules
  additionalLabels:
    "goog-gke-cluster": "trustram-gcp-cluster"
    "kubernetes.io/cluster/trustram-gcp-cluster": "owned"
    "trustram.io/environment": "production"
    "trustram.io/version": "v4.4"
    "trustram.io/multi-cloud": "true"
    "trustram.io/cost-optimization": "preemptible-instances"
  
  # Control Plane Load Balancer Configuration
  controlPlaneEndpoint:
    host: ""
    port: 6443
  
  # Failure Domains for High Availability
  failureDomains:
    us-west1-a:
      controlPlane: true
    us-west1-b:
      controlPlane: true
    us-west1-c:
      controlPlane: true

---
# Control Plane Configuration
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: trustram-gcp-control-plane
  namespace: default
spec:
  # Control Plane Replica Configuration
  replicas: 3
  version: v1.28.2
  
  # Machine Template Reference
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: GCPMachineTemplate
      name: gcp-control-plane-machine-template
  
  # Kubeadm Configuration
  kubeadmConfigSpec:
    # Cluster Configuration
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: gce
          enable-admission-plugins: NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook
          audit-log-maxage: "30"
          audit-log-maxbackup: "3"
          audit-log-maxsize: "100"
          audit-log-path: /var/log/audit.log
          feature-gates: "CSIMigration=true,CSIMigrationGCE=true"
        timeoutForControlPlane: 20m
      controllerManager:
        extraArgs:
          cloud-provider: gce
          allocate-node-cidrs: "true"
          cluster-cidr: "192.168.0.0/16"
          feature-gates: "CSIMigration=true,CSIMigrationGCE=true"
      etcd:
        local:
          dataDir: /var/lib/etcd
          extraArgs:
            listen-metrics-urls: http://0.0.0.0:2381
            quota-backend-bytes: "8589934592"  # 8GB
            auto-compaction-retention: "8"
      kubernetesVersion: v1.28.2
      controlPlaneEndpoint: ""
      networking:
        podSubnet: 192.168.0.0/16
        serviceSubnet: 10.96.0.0/12
      dns:
        type: CoreDNS
    
    # Init Configuration
    initConfiguration:
      nodeRegistration:
        criSocket: unix:///var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cloud-provider: gce
          provider-id: gce://trustram-v44-project/us-west1/INSTANCE_NAME
    
    # Join Configuration
    joinConfiguration:
      nodeRegistration:
        criSocket: unix:///var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cloud-provider: gce
          provider-id: gce://trustram-v44-project/us-west1/INSTANCE_NAME
    
    # Pre and Post Kubeadm Commands
    preKubeadmCommands:
    - hostnamectl set-hostname "$(curl -s 'http://metadata.google.internal/computeMetadata/v1/instance/name' -H 'Metadata-Flavor: Google')"
    - echo "127.0.0.1 $(curl -s 'http://metadata.google.internal/computeMetadata/v1/instance/name' -H 'Metadata-Flavor: Google')" >> /etc/hosts
    
    postKubeadmCommands:
    # Install GCP Cloud Controller Manager
    - |
      kubectl apply -f - <<EOF
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: cloud-controller-manager
        namespace: kube-system
        labels:
          k8s-app: cloud-controller-manager
      spec:
        selector:
          matchLabels:
            k8s-app: cloud-controller-manager
        template:
          metadata:
            labels:
              k8s-app: cloud-controller-manager
          spec:
            serviceAccountName: cloud-controller-manager
            containers:
            - name: cloud-controller-manager
              image: k8s.gcr.io/cloud-controller-manager:v1.28.2
              command:
              - /usr/local/bin/cloud-controller-manager
              - --cloud-provider=gce
              - --leader-elect=true
              - --use-service-account-credentials
              - --allocate-node-cidrs=true
              - --cluster-cidr=192.168.0.0/16
              volumeMounts:
              - name: etc-ssl
                mountPath: /etc/ssl
                readOnly: true
              - name: etc-ca-certificates
                mountPath: /etc/ca-certificates
                readOnly: true
            hostNetwork: true
            volumes:
            - name: etc-ssl
              hostPath:
                path: /etc/ssl
            - name: etc-ca-certificates
              hostPath:
                path: /etc/ca-certificates
            tolerations:
            - key: node.cloudprovider.kubernetes.io/uninitialized
              value: "true"
              effect: NoSchedule
            - key: node-role.kubernetes.io/control-plane
              effect: NoSchedule
      EOF
    
    # Install GCP Compute Persistent Disk CSI Driver
    - kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/master/deploy/kubernetes/manifests/gcp-compute-persistent-disk-csi-driver.yaml
    
    # Install Calico CNI
    - kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml
    
    # Configure Calico for GCP
    - |
      kubectl patch felixconfiguration default --type merge --patch='{"spec":{"ipipEnabled":false,"vxlanEnabled":true,"vxlanVNI":4096}}'
    
    # Install Google Cloud Load Balancer Controller
    - |
      kubectl apply -f - <<EOF
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: cloud-provider-gcp
        namespace: kube-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: system:cloud-provider-gcp
      rules:
      - apiGroups: [""]
        resources: ["events"]
        verbs: ["create", "patch", "update"]
      - apiGroups: [""]
        resources: ["nodes"]
        verbs: ["*"]
      - apiGroups: [""]
        resources: ["nodes/status"]
        verbs: ["patch"]
      - apiGroups: [""]
        resources: ["services"]
        verbs: ["list", "patch", "update", "watch"]
      - apiGroups: [""]
        resources: ["services/status"]
        verbs: ["list", "patch", "update", "watch"]
      - apiGroups: [""]
        resources: ["serviceaccounts"]
        verbs: ["create", "get", "list"]
      - apiGroups: [""]
        resources: ["persistentvolumes"]
        verbs: ["get", "list", "update", "create", "delete"]
      - apiGroups: [""]
        resources: ["persistentvolumeclaims"]
        verbs: ["get", "list", "watch", "update"]
      - apiGroups: ["storage.k8s.io"]
        resources: ["storageclasses"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["coordination.k8s.io"]
        resources: ["leases"]
        verbs: ["create", "get", "list", "watch", "update"]
      - apiGroups: ["discovery.k8s.io"]
        resources: ["endpointslices"]
        verbs: ["get", "list", "watch"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: system:cloud-provider-gcp
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: system:cloud-provider-gcp
      subjects:
      - kind: ServiceAccount
        name: cloud-provider-gcp
        namespace: kube-system
      EOF

---
# Worker Node Machine Deployment
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: trustram-gcp-worker-deployment
  namespace: default
  labels:
    cluster.x-k8s.io/cluster-name: trustram-gcp-cluster
spec:
  clusterName: trustram-gcp-cluster
  replicas: 3
  
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: trustram-gcp-cluster
      cluster.x-k8s.io/deployment-name: trustram-gcp-worker-deployment
  
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: trustram-gcp-cluster
        cluster.x-k8s.io/deployment-name: trustram-gcp-worker-deployment
    spec:
      clusterName: trustram-gcp-cluster
      version: v1.28.2
      
      # Failure Domain for High Availability
      failureDomain: "us-west1-a"
      
      # Infrastructure Template
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: GCPMachineTemplate
        name: gcp-worker-machine-template
      
      # Bootstrap Configuration
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: trustram-gcp-worker-bootstrap

---
# Worker Bootstrap Configuration Template
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: trustram-gcp-worker-bootstrap
  namespace: default
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          criSocket: unix:///var/run/containerd/containerd.sock
          kubeletExtraArgs:
            cloud-provider: gce
            provider-id: gce://trustram-v44-project/us-west1/INSTANCE_NAME
      
      preKubeadmCommands:
      - hostnamectl set-hostname "$(curl -s 'http://metadata.google.internal/computeMetadata/v1/instance/name' -H 'Metadata-Flavor: Google')"
      - echo "127.0.0.1 $(curl -s 'http://metadata.google.internal/computeMetadata/v1/instance/name' -H 'Metadata-Flavor: Google')" >> /etc/hosts
      
      postKubeadmCommands:
      # Configure node for GCP-specific optimizations
      - |
        # Configure node labels
        kubectl label node $(hostname) node.kubernetes.io/instance-type=$(curl -s 'http://metadata.google.internal/computeMetadata/v1/instance/machine-type' -H 'Metadata-Flavor: Google' | cut -d'/' -f4)
        kubectl label node $(hostname) topology.kubernetes.io/zone=$(curl -s 'http://metadata.google.internal/computeMetadata/v1/instance/zone' -H 'Metadata-Flavor: Google' | cut -d'/' -f4)
        kubectl label node $(hostname) topology.kubernetes.io/region=$(curl -s 'http://metadata.google.internal/computeMetadata/v1/instance/zone' -H 'Metadata-Flavor: Google' | cut -d'/' -f4 | sed 's/-[a-z]$//')

---
# GCP Managed Machine Pool for Auto-scaling
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: trustram-gcp-worker-pool
  namespace: default
  labels:
    cluster.x-k8s.io/cluster-name: trustram-gcp-cluster
spec:
  clusterName: trustram-gcp-cluster
  replicas: 3
  
  template:
    spec:
      clusterName: trustram-gcp-cluster
      version: v1.28.2
      
      # Infrastructure Template
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: GCPManagedMachinePool
        name: gcp-worker-managed-pool
      
      # Bootstrap Configuration
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfig
          name: trustram-gcp-worker-pool-bootstrap

---
# Worker Pool Bootstrap Configuration
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfig
metadata:
  name: trustram-gcp-worker-pool-bootstrap
  namespace: default
spec:
  joinConfiguration:
    nodeRegistration:
      criSocket: unix:///var/run/containerd/containerd.sock
      kubeletExtraArgs:
        cloud-provider: gce
        provider-id: gce://trustram-v44-project/us-west1/INSTANCE_NAME
  
  preKubeadmCommands:
  - hostnamectl set-hostname "$(curl -s 'http://metadata.google.internal/computeMetadata/v1/instance/name' -H 'Metadata-Flavor: Google')"
  - echo "127.0.0.1 $(curl -s 'http://metadata.google.internal/computeMetadata/v1/instance/name' -H 'Metadata-Flavor: Google')" >> /etc/hosts

---
# Firewall Rules for GCP Cluster
apiVersion: v1
kind: ConfigMap
metadata:
  name: gcp-firewall-rules
  namespace: default
data:
  firewall-rules.yaml: |
    # These firewall rules should be created in GCP Console or via gcloud CLI
    # gcloud compute firewall-rules create trustram-cluster-allow-internal \
    #   --allow tcp,udp,icmp \
    #   --source-ranges 10.2.0.0/16 \
    #   --target-tags trustram-cluster
    
    # gcloud compute firewall-rules create trustram-cluster-allow-apiserver \
    #   --allow tcp:6443 \
    #   --source-ranges 0.0.0.0/0 \
    #   --target-tags trustram-control-plane
    
    # gcloud compute firewall-rules create trustram-cluster-allow-nodeports \
    #   --allow tcp:30000-32767 \
    #   --source-ranges 0.0.0.0/0 \
    #   --target-tags trustram-worker
    
    # gcloud compute firewall-rules create trustram-cluster-allow-health-checks \
    #   --allow tcp:10256 \
    #   --source-ranges 35.191.0.0/16,130.211.0.0/22 \
    #   --target-tags trustram-cluster
