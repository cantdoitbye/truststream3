# TrustStream v4.2 GitHub Actions CI/CD Pipeline
# Comprehensive integration testing pipeline for GitHub Actions
# Author: MiniMax Agent
# Date: 2025-09-20
# Version: 1.0.0

name: TrustStream v4.2 Integration Testing Pipeline

on:
  push:
    branches: [ main, develop, 'feature/*', 'release/*' ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - integration
        - performance
        - regression
        - security
      environment:
        description: 'Target environment'
        required: false
        default: 'staging'
        type: choice
        options:
        - staging
        - production
        - isolated
      skip_performance:
        description: 'Skip performance tests'
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION: '18'
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
  SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
  DATABASE_URL: ${{ secrets.DATABASE_URL }}
  REDIS_URL: ${{ secrets.REDIS_URL }}
  
jobs:
  # ================================================================
  # PREPARATION AND SETUP
  # ================================================================
  
  prepare:
    name: 🚀 Prepare Environment
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      test-matrix: ${{ steps.setup-matrix.outputs.matrix }}
      should-run-performance: ${{ steps.setup-conditions.outputs.run-performance }}
      should-run-regression: ${{ steps.setup-conditions.outputs.run-regression }}
      deployment-environment: ${{ steps.setup-conditions.outputs.environment }}
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: 🔍 Detect Changes
      id: changes
      uses: dorny/paths-filter@v2
      with:
        filters: |
          core:
            - 'src/**'
            - 'supabase/**'
            - 'package.json'
            - 'package-lock.json'
          tests:
            - 'tests/**'
            - 'jest.config.js'
          performance:
            - 'src/trust-pyramid/**'
            - 'src/orchestrator/**'
          governance:
            - 'src/orchestrator/governance-orchestrator.ts'
            - 'src/agents/**'
          api:
            - 'src/api/**'
            - 'supabase/functions/**'
    
    - name: 🎯 Setup Test Conditions
      id: setup-conditions
      run: |
        # Determine which tests to run based on changes and inputs
        RUN_PERFORMANCE="false"
        RUN_REGRESSION="false"
        ENVIRONMENT="${{ github.event.inputs.environment || 'staging' }}"
        
        if [ "${{ steps.changes.outputs.performance }}" == "true" ] || [ "${{ github.event.inputs.test_suite }}" == "performance" ] || [ "${{ github.event.inputs.test_suite }}" == "all" ]; then
          if [ "${{ github.event.inputs.skip_performance }}" != "true" ]; then
            RUN_PERFORMANCE="true"
          fi
        fi
        
        if [ "${{ steps.changes.outputs.core }}" == "true" ] || [ "${{ github.event_name }}" == "schedule" ] || [ "${{ github.event.inputs.test_suite }}" == "regression" ] || [ "${{ github.event.inputs.test_suite }}" == "all" ]; then
          RUN_REGRESSION="true"
        fi
        
        # Set outputs
        echo "run-performance=$RUN_PERFORMANCE" >> $GITHUB_OUTPUT
        echo "run-regression=$RUN_REGRESSION" >> $GITHUB_OUTPUT
        echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
    
    - name: 🔧 Setup Test Matrix
      id: setup-matrix
      run: |
        # Create dynamic test matrix based on conditions
        MATRIX='{"include":[]}'
        
        # Always run basic integration tests
        MATRIX=$(echo $MATRIX | jq '.include += [{"type": "integration", "suite": "basic", "timeout": 30}]')
        
        # Add v4.1 compatibility tests
        MATRIX=$(echo $MATRIX | jq '.include += [{"type": "compatibility", "suite": "v4.1", "timeout": 20}]')
        
        # Add governance tests if governance changes detected
        if [ "${{ steps.changes.outputs.governance }}" == "true" ] || [ "${{ github.event.inputs.test_suite }}" == "all" ]; then
          MATRIX=$(echo $MATRIX | jq '.include += [{"type": "governance", "suite": "workflows", "timeout": 25}]')
          MATRIX=$(echo $MATRIX | jq '.include += [{"type": "governance", "suite": "agents", "timeout": 20}]')
        fi
        
        # Add API tests if API changes detected
        if [ "${{ steps.changes.outputs.api }}" == "true" ] || [ "${{ github.event.inputs.test_suite }}" == "all" ]; then
          MATRIX=$(echo $MATRIX | jq '.include += [{"type": "api", "suite": "contracts", "timeout": 15}]')
        fi
        
        # Add security tests
        MATRIX=$(echo $MATRIX | jq '.include += [{"type": "security", "suite": "baseline", "timeout": 25}]')
        
        echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
    
    - name: 📊 Display Test Plan
      run: |
        echo "## 🎯 Test Execution Plan" >> $GITHUB_STEP_SUMMARY
        echo "- **Environment:** ${{ steps.setup-conditions.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Performance Tests:** ${{ steps.setup-conditions.outputs.run-performance }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Regression Tests:** ${{ steps.setup-conditions.outputs.run-regression }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Test Matrix:** \`${{ steps.setup-matrix.outputs.matrix }}\`" >> $GITHUB_STEP_SUMMARY

  # ================================================================
  # INTEGRATION TESTS
  # ================================================================
  
  integration-tests:
    name: 🧪 Integration Tests
    runs-on: ubuntu-latest
    needs: prepare
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix: ${{fromJson(needs.prepare.outputs.test-matrix)}}
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: truststream_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
    
    - name: 🟢 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: 📦 Install Dependencies
      run: |
        npm ci
        npm run build
    
    - name: 🗄️ Setup Test Database
      run: |
        # Install Supabase CLI
        npm install -g @supabase/cli
        
        # Setup test database schema
        export PGPASSWORD=postgres
        psql -h localhost -U postgres -d truststream_test -f tests/fixtures/test-schema.sql
        
        # Seed test data
        npm run test:seed
    
    - name: 🔧 Configure Test Environment
      run: |
        # Create test environment configuration
        cat > .env.test << EOF
        NODE_ENV=test
        SUPABASE_URL=${{ env.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY=${{ env.SUPABASE_SERVICE_ROLE_KEY }}
        DATABASE_URL=postgresql://postgres:postgres@localhost:5432/truststream_test
        REDIS_URL=redis://localhost:6379
        TEST_SUITE_TYPE=${{ matrix.type }}
        TEST_SUITE_NAME=${{ matrix.suite }}
        GITHUB_RUN_ID=${{ github.run_id }}
        GITHUB_SHA=${{ github.sha }}
        EOF
    
    - name: 🏃 Run Integration Tests
      timeout-minutes: ${{ matrix.timeout }}
      run: |
        # Set test environment
        export NODE_ENV=test
        
        # Run specific test suite based on matrix
        case "${{ matrix.type }}" in
          "integration")
            npm run test:integration:${{ matrix.suite }}
            ;;
          "compatibility")
            npm run test:compatibility:${{ matrix.suite }}
            ;;
          "governance")
            npm run test:governance:${{ matrix.suite }}
            ;;
          "api")
            npm run test:api:${{ matrix.suite }}
            ;;
          "security")
            npm run test:security:${{ matrix.suite }}
            ;;
          *)
            echo "Unknown test type: ${{ matrix.type }}"
            exit 1
            ;;
        esac
    
    - name: 📊 Collect Test Results
      if: always()
      run: |
        # Collect test results and artifacts
        mkdir -p test-results/${{ matrix.type }}-${{ matrix.suite }}
        
        # Copy test results
        if [ -f test-results.json ]; then
          cp test-results.json test-results/${{ matrix.type }}-${{ matrix.suite }}/
        fi
        
        # Copy coverage reports
        if [ -d coverage ]; then
          cp -r coverage test-results/${{ matrix.type }}-${{ matrix.suite }}/
        fi
        
        # Copy logs
        if [ -d logs ]; then
          cp -r logs test-results/${{ matrix.type }}-${{ matrix.suite }}/
        fi
    
    - name: 📤 Upload Test Results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.type }}-${{ matrix.suite }}
        path: test-results/
        retention-days: 30
    
    - name: 📈 Generate Test Report
      if: always()
      run: |
        # Generate test summary for GitHub
        echo "## 🧪 ${{ matrix.type }} - ${{ matrix.suite }} Test Results" >> $GITHUB_STEP_SUMMARY
        
        if [ -f test-results.json ]; then
          TOTAL_TESTS=$(jq '.numTotalTests' test-results.json)
          PASSED_TESTS=$(jq '.numPassedTests' test-results.json)
          FAILED_TESTS=$(jq '.numFailedTests' test-results.json)
          
          echo "- **Total Tests:** $TOTAL_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed:** ✅ $PASSED_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed:** ❌ $FAILED_TESTS" >> $GITHUB_STEP_SUMMARY
          
          if [ "$FAILED_TESTS" -gt 0 ]; then
            echo "- **Status:** ❌ FAILED" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Status:** ✅ PASSED" >> $GITHUB_STEP_SUMMARY
          fi
        fi

  # ================================================================
  # PERFORMANCE TESTS
  # ================================================================
  
  performance-tests:
    name: ⚡ Performance Tests
    runs-on: ubuntu-latest
    needs: [prepare, integration-tests]
    if: needs.prepare.outputs.should-run-performance == 'true'
    timeout-minutes: 60
    
    strategy:
      fail-fast: false
      matrix:
        test-scenario: [
          { name: "governance-scoring", users: 50, duration: 300 },
          { name: "trust-calculation", users: 100, duration: 180 },
          { name: "agent-coordination", users: 25, duration: 240 },
          { name: "memory-operations", users: 75, duration: 200 }
        ]
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
    
    - name: 🟢 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: 📦 Install Dependencies
      run: |
        npm ci
        npm run build
    
    - name: 🔧 Setup Performance Environment
      run: |
        # Configure for performance testing
        cat > .env.performance << EOF
        NODE_ENV=performance
        SUPABASE_URL=${{ env.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY=${{ env.SUPABASE_SERVICE_ROLE_KEY }}
        PERFORMANCE_TEST_SCENARIO=${{ matrix.test-scenario.name }}
        VIRTUAL_USERS=${{ matrix.test-scenario.users }}
        TEST_DURATION_SECONDS=${{ matrix.test-scenario.duration }}
        GITHUB_RUN_ID=${{ github.run_id }}
        EOF
    
    - name: 🏃 Run Performance Tests
      timeout-minutes: 45
      run: |
        # Run performance test scenario
        npm run test:performance -- \
          --scenario=${{ matrix.test-scenario.name }} \
          --users=${{ matrix.test-scenario.users }} \
          --duration=${{ matrix.test-scenario.duration }} \
          --output=performance-results-${{ matrix.test-scenario.name }}.json
    
    - name: 📊 Analyze Performance Results
      run: |
        # Analyze performance results and generate report
        npm run analyze:performance -- \
          --input=performance-results-${{ matrix.test-scenario.name }}.json \
          --baseline=performance-baselines/${{ matrix.test-scenario.name }}.json \
          --output=performance-analysis-${{ matrix.test-scenario.name }}.json
    
    - name: 📤 Upload Performance Results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: performance-results-${{ matrix.test-scenario.name }}
        path: |
          performance-results-*.json
          performance-analysis-*.json
        retention-days: 90
    
    - name: 📈 Generate Performance Report
      if: always()
      run: |
        echo "## ⚡ Performance Test Results - ${{ matrix.test-scenario.name }}" >> $GITHUB_STEP_SUMMARY
        
        if [ -f performance-analysis-${{ matrix.test-scenario.name }}.json ]; then
          AVG_RESPONSE_TIME=$(jq '.metrics.response_times.average_ms' performance-analysis-${{ matrix.test-scenario.name }}.json)
          THROUGHPUT=$(jq '.metrics.throughput.requests_per_second' performance-analysis-${{ matrix.test-scenario.name }}.json)
          ERROR_RATE=$(jq '.metrics.throughput.error_rate_percentage' performance-analysis-${{ matrix.test-scenario.name }}.json)
          
          echo "- **Virtual Users:** ${{ matrix.test-scenario.users }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration:** ${{ matrix.test-scenario.duration }}s" >> $GITHUB_STEP_SUMMARY
          echo "- **Avg Response Time:** ${AVG_RESPONSE_TIME}ms" >> $GITHUB_STEP_SUMMARY
          echo "- **Throughput:** ${THROUGHPUT} req/s" >> $GITHUB_STEP_SUMMARY
          echo "- **Error Rate:** ${ERROR_RATE}%" >> $GITHUB_STEP_SUMMARY
          
          # Check for performance regressions
          REGRESSION_DETECTED=$(jq '.regression_analysis.has_regression' performance-analysis-${{ matrix.test-scenario.name }}.json)
          if [ "$REGRESSION_DETECTED" == "true" ]; then
            echo "- **Status:** ⚠️ PERFORMANCE REGRESSION DETECTED" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Status:** ✅ PERFORMANCE WITHIN LIMITS" >> $GITHUB_STEP_SUMMARY
          fi
        fi

  # ================================================================
  # REGRESSION TESTS
  # ================================================================
  
  regression-tests:
    name: 🔄 Regression Tests
    runs-on: ubuntu-latest
    needs: [prepare, integration-tests]
    if: needs.prepare.outputs.should-run-regression == 'true'
    timeout-minutes: 90
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
    
    - name: 🟢 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: 📦 Install Dependencies
      run: |
        npm ci
        npm run build
    
    - name: 📥 Download Baseline Data
      run: |
        # Download regression baselines from artifacts or S3
        mkdir -p regression-baselines
        
        # Try to download from previous runs
        gh run download --name regression-baselines --dir regression-baselines || echo "No previous baselines found"
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    
    - name: 🔧 Setup Regression Environment
      run: |
        cat > .env.regression << EOF
        NODE_ENV=regression
        SUPABASE_URL=${{ env.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY=${{ env.SUPABASE_SERVICE_ROLE_KEY }}
        REGRESSION_BASELINE_PATH=regression-baselines
        GITHUB_RUN_ID=${{ github.run_id }}
        GITHUB_SHA=${{ github.sha }}
        GITHUB_REF=${{ github.ref }}
        EOF
    
    - name: 🏃 Run Regression Tests
      timeout-minutes: 75
      run: |
        # Run comprehensive regression test suite
        npm run test:regression -- \
          --suite=comprehensive \
          --baseline-path=regression-baselines \
          --output=regression-results.json \
          --generate-new-baselines=${{ github.ref == 'refs/heads/main' && 'true' || 'false' }}
    
    - name: 📊 Analyze Regression Results
      run: |
        # Analyze regression test results
        npm run analyze:regression -- \
          --input=regression-results.json \
          --output=regression-analysis.json \
          --threshold=5.0
    
    - name: 📤 Upload Regression Results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: regression-results
        path: |
          regression-results.json
          regression-analysis.json
          regression-baselines/
        retention-days: 180
    
    - name: 📈 Generate Regression Report
      if: always()
      run: |
        echo "## 🔄 Regression Test Results" >> $GITHUB_STEP_SUMMARY
        
        if [ -f regression-analysis.json ]; then
          TOTAL_TESTS=$(jq '.summary.total_tests' regression-analysis.json)
          REGRESSIONS_DETECTED=$(jq '.summary.regressions_detected' regression-analysis.json)
          CRITICAL_REGRESSIONS=$(jq '.summary.critical_regressions' regression-analysis.json)
          
          echo "- **Total Tests:** $TOTAL_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Regressions Detected:** $REGRESSIONS_DETECTED" >> $GITHUB_STEP_SUMMARY
          echo "- **Critical Regressions:** $CRITICAL_REGRESSIONS" >> $GITHUB_STEP_SUMMARY
          
          if [ "$CRITICAL_REGRESSIONS" -gt 0 ]; then
            echo "- **Status:** 🚨 CRITICAL REGRESSIONS DETECTED" >> $GITHUB_STEP_SUMMARY
          elif [ "$REGRESSIONS_DETECTED" -gt 0 ]; then
            echo "- **Status:** ⚠️ REGRESSIONS DETECTED" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Status:** ✅ NO REGRESSIONS DETECTED" >> $GITHUB_STEP_SUMMARY
          fi
        fi

  # ================================================================
  # TEST RESULTS AGGREGATION
  # ================================================================
  
  aggregate-results:
    name: 📊 Aggregate Test Results
    runs-on: ubuntu-latest
    needs: [prepare, integration-tests, performance-tests, regression-tests]
    if: always()
    timeout-minutes: 15
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
    
    - name: 🟢 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: 📦 Install Dependencies
      run: npm ci
    
    - name: 📥 Download All Test Results
      uses: actions/download-artifact@v4
      with:
        path: all-test-results/
    
    - name: 🔧 Aggregate Test Results
      run: |
        # Aggregate all test results into a comprehensive report
        npm run aggregate:test-results -- \
          --input-dir=all-test-results \
          --output=comprehensive-test-report.json \
          --format=github-summary
    
    - name: 📊 Generate Quality Metrics
      run: |
        # Calculate overall quality metrics
        npm run calculate:quality-metrics -- \
          --test-results=comprehensive-test-report.json \
          --output=quality-metrics.json
    
    - name: 📈 Generate Comprehensive Report
      run: |
        echo "# 🎯 TrustStream v4.2 Integration Test Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## 📊 Overall Test Summary" >> $GITHUB_STEP_SUMMARY
        
        if [ -f comprehensive-test-report.json ]; then
          TOTAL_TESTS=$(jq '.summary.total_tests' comprehensive-test-report.json)
          PASSED_TESTS=$(jq '.summary.passed_tests' comprehensive-test-report.json)
          FAILED_TESTS=$(jq '.summary.failed_tests' comprehensive-test-report.json)
          SUCCESS_RATE=$(jq '.summary.success_rate_percentage' comprehensive-test-report.json)
          
          echo "- **Total Tests Executed:** $TOTAL_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests Passed:** ✅ $PASSED_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests Failed:** ❌ $FAILED_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Success Rate:** $SUCCESS_RATE%" >> $GITHUB_STEP_SUMMARY
          
          # Add quality metrics if available
          if [ -f quality-metrics.json ]; then
            QUALITY_SCORE=$(jq '.overall_quality_score' quality-metrics.json)
            PERFORMANCE_SCORE=$(jq '.performance_score' quality-metrics.json)
            RELIABILITY_SCORE=$(jq '.reliability_score' quality-metrics.json)
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## 🏆 Quality Metrics" >> $GITHUB_STEP_SUMMARY
            echo "- **Overall Quality Score:** $QUALITY_SCORE/100" >> $GITHUB_STEP_SUMMARY
            echo "- **Performance Score:** $PERFORMANCE_SCORE/100" >> $GITHUB_STEP_SUMMARY
            echo "- **Reliability Score:** $RELIABILITY_SCORE/100" >> $GITHUB_STEP_SUMMARY
          fi
        fi
        
        # Add recommendations
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## 🎯 Recommendations" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.integration-tests.result }}" == "failure" ]; then
          echo "- ❌ **Integration tests failed** - Review test failures and fix issues before merging" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.performance-tests.result }}" == "failure" ]; then
          echo "- ⚠️ **Performance regressions detected** - Review performance impact and optimize if necessary" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.regression-tests.result }}" == "failure" ]; then
          echo "- 🚨 **Regression tests failed** - Critical regressions detected, immediate attention required" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.integration-tests.result }}" == "success" ] && [ "${{ needs.performance-tests.result }}" != "failure" ] && [ "${{ needs.regression-tests.result }}" != "failure" ]; then
          echo "- ✅ **All tests passed successfully** - Ready for deployment" >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: 📤 Upload Comprehensive Report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: comprehensive-test-report
        path: |
          comprehensive-test-report.json
          quality-metrics.json
        retention-days: 365
    
    - name: 💬 Post Results to PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // Read comprehensive report
          let reportContent = '## 🎯 TrustStream v4.2 Integration Test Results\n\n';
          
          try {
            const report = JSON.parse(fs.readFileSync('comprehensive-test-report.json', 'utf8'));
            
            reportContent += `### 📊 Test Summary\n`;
            reportContent += `- **Total Tests:** ${report.summary.total_tests}\n`;
            reportContent += `- **Passed:** ✅ ${report.summary.passed_tests}\n`;
            reportContent += `- **Failed:** ❌ ${report.summary.failed_tests}\n`;
            reportContent += `- **Success Rate:** ${report.summary.success_rate_percentage}%\n\n`;
            
            // Add status badge
            if (report.summary.success_rate_percentage >= 95) {
              reportContent += '### 🟢 Status: READY FOR MERGE\n\n';
            } else if (report.summary.success_rate_percentage >= 80) {
              reportContent += '### 🟡 Status: REVIEW REQUIRED\n\n';
            } else {
              reportContent += '### 🔴 Status: FAILING - DO NOT MERGE\n\n';
            }
            
          } catch (error) {
            reportContent += '❌ Failed to load comprehensive test report\n\n';
          }
          
          // Post comment
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: reportContent
          });

  # ================================================================
  # DEPLOYMENT PREPARATION
  # ================================================================
  
  prepare-deployment:
    name: 🚀 Prepare Deployment
    runs-on: ubuntu-latest
    needs: [aggregate-results]
    if: github.ref == 'refs/heads/main' && needs.aggregate-results.result == 'success'
    timeout-minutes: 10
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
    
    - name: 📥 Download Test Results
      uses: actions/download-artifact@v4
      with:
        name: comprehensive-test-report
        path: test-reports/
    
    - name: ✅ Validate Deployment Readiness
      run: |
        # Validate that all tests passed and system is ready for deployment
        if [ -f test-reports/comprehensive-test-report.json ]; then
          SUCCESS_RATE=$(jq '.summary.success_rate_percentage' test-reports/comprehensive-test-report.json)
          
          if [ "$SUCCESS_RATE" -ge 95 ]; then
            echo "✅ System ready for deployment (Success rate: $SUCCESS_RATE%)"
            echo "DEPLOYMENT_READY=true" >> $GITHUB_ENV
          else
            echo "❌ System not ready for deployment (Success rate: $SUCCESS_RATE%)"
            echo "DEPLOYMENT_READY=false" >> $GITHUB_ENV
            exit 1
          fi
        else
          echo "❌ Test report not found"
          exit 1
        fi
    
    - name: 🏷️ Create Deployment Tag
      if: env.DEPLOYMENT_READY == 'true'
      run: |
        # Create deployment tag
        TIMESTAMP=$(date +%Y%m%d-%H%M%S)
        TAG_NAME="deployment-$TIMESTAMP"
        
        git config user.name "GitHub Actions"
        git config user.email "actions@github.com"
        git tag -a "$TAG_NAME" -m "Deployment tag - All tests passed"
        git push origin "$TAG_NAME"
        
        echo "DEPLOYMENT_TAG=$TAG_NAME" >> $GITHUB_ENV
    
    - name: 📋 Generate Deployment Manifest
      if: env.DEPLOYMENT_READY == 'true'
      run: |
        # Generate deployment manifest
        cat > deployment-manifest.json << EOF
        {
          "deployment_tag": "${{ env.DEPLOYMENT_TAG }}",
          "commit_sha": "${{ github.sha }}",
          "test_results": {
            "comprehensive_report": "test-reports/comprehensive-test-report.json",
            "deployment_ready": true,
            "validation_timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
        }
        EOF
    
    - name: 📤 Upload Deployment Artifacts
      if: env.DEPLOYMENT_READY == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: deployment-artifacts
        path: |
          deployment-manifest.json
          test-reports/
        retention-days: 365

# ================================================================
# REUSABLE WORKFLOWS AND ADDITIONAL CONFIGURATIONS
# ================================================================

# Additional workflow configurations for different scenarios:
# - Hotfix deployment pipeline
# - Feature branch validation
# - Nightly comprehensive testing
# - Production health checks
# - Security scanning integration
